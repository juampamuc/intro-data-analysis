# (PART) Preliminaries {-}

# General Introduction

<hr>

This chapter lays out the learning goals of this book (Section \@ref(Chap-01-00-intro-learning-goals)) and describes how these goals are to be achieved (Section \@ref(Chap-01-00-intro-course-structure)). Sections \@ref(Chap-01-00-intro-tools) and \@ref(Chap-01-00-intro-topics) detail which technical tools and theoretical concepts are covered in the course, and which are not. There will be some information on the kinds of data sets we will use during the course in Section \@ref(Chap-01-00-intro-data-sets). Section \@ref(Chap-01-00-intro-installation) provides information about how to install the necessary tools for this course. Finally, Section \@ref(Chap-01-00-intro-schedule) provides more information on how a 12-week course can be structured based on the material in this web-book.

## Learning goals {#Chap-01-00-intro-learning-goals}

At the end of this course students should:

- have gained the competence to
  - understand complex data sets,
  - manipulate a data set (using R), so as to
  - plot aspects of it in ways that are useful for answering a given research question
- understand the general logic of statistical inference, in particular
  - be able to interpret and apply standard analyses from a frequentist and a Bayesian approach
- be able to independently evaluate statistical analyses based on their adequacy for a given research question and data set
- be able to critically assess the adequacy of analyses commonly found in the literature

Notice that this is, although a lot of hard work already, still rather modest! It doesn't actually say that we necessarily aim at the competence to *do it* or even to *do it flawlessly*! **Our main goal is understanding**, because that is the foundation of practical success *and* the foundation of an ability to learn more in the future. **We do not teach tricks! We do not share recipes!**

## Course structure {#Chap-01-00-intro-course-structure}

The course consists of five parts. After giving a more detailed overview of the course in this chapter, Part I introduces R, the main programming language that we will use. Part II covers what is often called *descriptive statistics*. It also gives us room to learn more about R when we massage data into shape, compute summary statistics, and plot different data types in different ways. Part III covers the basic theoretical concepts of *Bayesian data analysis*. Part IV introduces regression modeling. Part V introduces basic ideas from *frequentist data analysis* and compares the frequentist and the Bayesian approach. 

A number of characteristic features distinguishes this course from the bulk of its cousins out there. 

1. First, we use a **model-centric approach**, i.e., we are going to explicitly represent and talk about statistical models as a formalized set of the assumptions which underly a specific analysis.
2. Second, we will use a **computational approach**, i.e., we foster an understanding of mathematical notions with computer simulations or other variants of helpful code. 
3. Third, this course takes a **dual approach** in that it introduces both the frequentist and the Bayesian approach to statistical inference. We will start off with the Bayesian approach, because it is arguably more intuitive. Yet, a model-centric Bayesian approach also helps with understanding basic concepts from the frequentist paradigm.
4. Fourth, the course focuses on **generalized linear models**, a class of models that have become the new standard for analyses of experimental data in the social and psychological sciences. They are also very useful for data exploration in other domains (such as machine learning).

There are also appendices with additional information:

- Further useful material (textbooks, manuals, etc.) is provided in Appendix \@ref(app-90-further-material).
- Appendix \@ref(app-91-distributions) covers the most important probability distributions used in this book.
- An excursion providing more information about the important Exponential Family of probability distributions and the Maximum Entropy Principle is given in Appendix \@ref(app-92-exponential-family).
- The data sets which reoccur throughout the book as "running examples" are succinctly summarized in Appendix \@ref(app-93-data-sets).
- Appendix \@ref(app-94-open-science) surveys and motivates ideas for good scientific practice and principles of open science.

## Tools used in this course {#Chap-01-00-intro-tools}

The main programming language used in this course is [R](https://www.R-project.org/) [@R2018]. We will make heavy use of the `tidyverse` package [@tidyverse2017], which provides a unified set of functions and conventions that deviate (sometimes: substantially) from basic R. We will also be using the [probabilistic programming language WebPPL](http://webppl.org/) [@dippl], but only "passively" in order to quickly obtain results from probabilistic calculations that we can experiment with directly in the browser in order to better understand certain ideas or concepts. You will not need to learn to write WebPPL code yourself.

We will rely on the R package [`brms`](https://github.com/paul-buerkner/brms) [@brms2017] for running Bayesian generalized regression models, which itself relies on the probabilistic programming language [Stan](https://mc-stan.org/) [@stan2017]. We will, however, not learn to use Stan in this course, but we *will* take a glimpse at some Stan code to have seen, at least roughly, where the Markov Chain Monte Carlo samples come from which we will use.

Section \@ref(Chap-01-00-intro-installation) gives information about how to install the tools necessary for this course.

## Topics covered (and not covered) in the course {#Chap-01-00-intro-topics}

The main topics that this course will cover are:

+ **data preparation**: how to clean up and massage a data set into an appropriate shape for plotting and analysis

+ **data visualization**: how to select relevant aspects of data for informative visualization

+ **statistical models**: what that is, and why it's beneficial to think in terms of models, not tests

+ **statistical inference**: what that is, and how it's done in frequentist and Bayesian approaches

+ **hypothesis testing**: how to test assumptions about a model's parameters

+ **generalized regression**: how to apply generalized regression models to different types of data sets

There is, obviously, a lot that we will *not* cover in this course. We will, for instance, not dwell at any length on the specifics of algorithms for computing statistical inferences or model fits. We will also only deal with the history of statistics and questions of philosophy of science at the very end of this course and only to the extent that it helps us to better understand the theoretical notions and practical habits that are important in the context of this course. We will also not do extremely heavy math.

There are at least two different motivations for data analysis, and it is important to keep them apart. This course focuses on **data analysis for explanation**, i.e., routines that help us understand reality through inspection of empirical data. We will only glance at the alternative approach, which is **data analysis for prediction**, i.e., using models to predict future observations, as commonly practiced in machine learning and its applications. In sloppy slogan form, this course treats data science for scientific knowledge gain, not data science as an engineering application.

## Data sets covered {#Chap-01-00-intro-data-sets}

Data analysis can be quite varied because data itself can be quite varied. We try to present some variation, but since this is an introductory course with lots of other ground to cover, we will be slightly conservative in the kind of data that we analyze. The focus is on data from behavioral experiments (like categorical choices or reaction times). There will be no analyses specifically tailored to pictures, sounds, dates or time sequences in this course. Appendix \@ref(app-93-data-sets) gives an overview of the most important, recurring data sets used in this course.

Most of the data sets that we will use repeatedly in this class come from various psychological experiments. To make this even more immersive, these experiments are implemented as browser-based experiments, using [_magpie](https://magpie-ea.github.io/magpie-site/index.html). This makes it possible for students of this course to actually conduct the exact experiments whose data the book will analyze (and maybe generate some more intuitions and some hypotheses).

More on data sets used in this book is provided in Appendix \@ref(app-93-data-sets).

<!-- But it also makes it possible that we will analyze ourselves. That's why part of the exercises for this course will run additional analyses on data collected from the aspiring data analysts themselves. If you want to become an analyst, you should also have undergone analysis yourself, so to speak. -->

## Installation {#Chap-01-00-intro-installation}

To follow the code in this book, you need a recent version of [R](https://www.r-project.org/)
(recommended is an R version at least 4.0). We recommend the use of
[RStudio](https://rstudio.com/).

The course also has its own R package. The `aida` package contains convenience functions introduced and used in this book. It can be installed by executing the following code. (More on R, packages and their installation in the next chapter.) 

```{r eval = F}
install.packages('remotes')
remotes::install_github('michael-franke/aida-package')
```

This course also requires the packages `rstan` and `brms` which let R interact with the probabilistic programming language Stan. Installation of these packages can be difficult. If you run into trouble during the installation of these packages, please follow the instructions on the [Stan homepage](https://mc-stan.org/) for the most recent recommendations for installation for your OS.


## Example schedule (12-week course) {#Chap-01-00-intro-schedule}

It is possible to teach the content of this web-book in a semester (12 weeks).
Below is an example schedule of how the material can be chunked.
This schedule assumes that sections marked as "excursions" are skipped and that lecturers give clear guidance as to which aspects of each chapter are most important.

```{r ch-01-00-schedule, echo = F}
table_data <- tribble(
  ~week, ~topic, ~chapter,
  1,  "overiew, installation, first steps in R"  , "1 & 2.1",
  2,  "basics of R", "2",
  3,  "data types & wrangling", "3 & 4",
  4,  "summary statistics & plotting", "5 & 6",
  5,  "probability basics", "7" ,
  6,  "models & parameter inference", "8 & 9",
  7,  "model comparison & hypothesis testing", "10 & 11",
  8,  "linear regression", "12 & 13",
  9,  "categorical predictors & GLM", "14 & 15",
  10, "frequentist statistics", "16",
  11, "Bayes vs frequentist", "17",
  12, "discussion / reflection", ""
)
knitr::kable(
  table_data,
  escape = F,
  caption = "Example schedule for a 12-week course based on this web-book.", 
  booktabs = TRUE
)
```